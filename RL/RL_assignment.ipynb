{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e1tERPp82B1"
   },
   "source": [
    "Empty video folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mA_X31N88dZm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "files = glo>b.glob('./video/*')\n",
    "for f in files:\n",
    "  os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "**Initialise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TZefME0MTvA"
   },
   "outputs": [],
   "source": [
    "# you can write a brief 8-10 line abstract detailing your submission and experiments here\n",
    "\n",
    "# Using RAM version of gravitar as the frame buffer version used too much memory when using the replay buffer\n",
    "# to get any meaningful results\n",
    "# DQN using target network, dueling, prioritised replay, and noisy networks with epsilon greedy decay as well as it gives seemingly better results in practice\n",
    "# Using noisy networks and dueling despite Rainbow DQN paper (arXiv:1710.02298) reporting worse peformance\n",
    "# on gravitar my tests showed better convergence on the RAM version\n",
    "\n",
    "# Results show that the model is converging a little under a score of 2000 likely as this is a full clear of planet and anything it learns to help planet 2 harm performance on planet 1 \n",
    "\n",
    "# The code is also written to run on a GPU using CUDA so please select a GPU runtime on google colab when running or ensure CUDA is available on NCC\n",
    "\n",
    "# the code is based on https://github.com/seungeunrho/minimalRL/blob/master/dqn.py, which is released under the MIT licesne\n",
    "# make sure you reference any code you have studied as above, with one comment line per reference\n",
    "\n",
    "# imports\n",
    "import gym\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# hyperparameters\n",
    "learning_rate = 1e-4\n",
    "gamma         = 0.98\n",
    "buffer_limit  = 100000\n",
    "batch_size    = 32\n",
    "video_every   = 25\n",
    "print_every   = 5\n",
    "ep_limit = 1e6\n",
    "update_freq = 1000\n",
    "\n",
    "\n",
    "# Class is adapted from https://github.com/Shmuma/ptan/blob/master/samples/rainbow/lib/dqn_model.py which is released under the MIT license\n",
    "class Noisy(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, sig_init=0.4, bias=True):\n",
    "        super(Noisy, self).__init__(in_features, out_features, bias)\n",
    "        self.sig_weight = nn.Parameter(torch.Tensor(out_features, in_features).fill_(sig_init/math.sqrt(in_features)))\n",
    "        self.register_buffer('weight_var', torch.zeros(out_features, in_features))\n",
    "        if bias == True:\n",
    "            self.sig_bias = nn.Parameter(torch.Tensor(out_features).fill_(sig_init/math.sqrt(out_features)))\n",
    "            self.register_buffer('bias_var', torch.zeros(out_features))\n",
    "        self.reset_params()\n",
    "      \n",
    "    def reset_params(self):\n",
    "        dev = 1/math.sqrt(self.sig_weight.size(1))\n",
    "        self.weight.data.uniform_(-dev, dev)\n",
    "        self.bias.data.uniform_(-dev, dev)\n",
    "\n",
    "    \n",
    "    def forward(self, ins):\n",
    "        torch.randn(self.weight_var.size(), out=self.weight_var)\n",
    "        tmp = self.bias\n",
    "        if self.bias is not None:\n",
    "            torch.randn(self.bias_var.size(), out=self.bias_var)\n",
    "            tmp = tmp + self.sig_bias * Variable(self.bias_var).cuda()\n",
    "        return F.linear(ins, self.weight+self.sig_weight * Variable(self.weight_var).cuda(), tmp)\n",
    "\n",
    "\n",
    "\n",
    "# Class adapted from https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb there was no license in the repo\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = []\n",
    "        self.alpha = 0.5\n",
    "        self.cap = buffer_limit\n",
    "        self.pos = 0\n",
    "        self.prios = np.zeros((self.cap,), dtype=np.float32)\n",
    "    \n",
    "    def put(self, transition):\n",
    "        max_prio = self.prios.max() if self.buffer else 1.0\n",
    "        if len(self.buffer) < self.cap:\n",
    "            self.buffer.append(transition)\n",
    "        else:\n",
    "            self.buffer[self.pos] = transition\n",
    "        # add new transition to buff with max priority\n",
    "        self.prios[self.pos] = max_prio\n",
    "        self.pos = (self.pos+1) % self.cap\n",
    "\n",
    "    def sample(self, n):\n",
    "        if len(self.buffer) == self.cap:\n",
    "            prios = self.prios\n",
    "        else:\n",
    "            prios = self.prios[:self.pos]\n",
    "        # Scale priorites into probabilities\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        # sample batch from buffer using priorities\n",
    "        idx = np.random.choice(len(self.buffer), n, p=probs)\n",
    "        mini_batch = [self.buffer[id] for id in idx]\n",
    "        # using fixed exponent of 0.5 rather than variable as performance is better\n",
    "        weights = 1/np.sqrt(len(self.buffer) * probs[idx])\n",
    "        weights /= weights.max()\n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        # create sample for model\n",
    "        for transition in mini_batch:\n",
    "          # s = state, a = action, r = reward, s_prime = new state\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "        # return samples on device\n",
    "        return torch.tensor(s_lst, dtype=torch.float).to(device), torch.tensor(a_lst).to(device), \\\n",
    "               torch.tensor(r_lst).to(device), torch.tensor(s_prime_lst, dtype=torch.float).to(device), \\\n",
    "               torch.tensor(done_mask_lst).to(device), idx, \\\n",
    "               torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "    def update(self, indices, priorities):\n",
    "        for id, prio in zip(indices, priorities):\n",
    "            self.prios[id] = prio\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.pre = nn.Sequential(\n",
    "            nn.Linear(np.array(env.observation_space.shape).prod(), 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.adv = nn.Sequential(\n",
    "              Noisy(256, 84),\n",
    "              nn.ReLU(),\n",
    "              Noisy(84, env.action_space.n)\n",
    "        )\n",
    "        self.val = nn.Sequential(\n",
    "            Noisy(256, 84),\n",
    "            nn.ReLU(),\n",
    "            Noisy(84, 1)\n",
    "        )\n",
    "    # using dueling DQN so return value of taking action in this state and the state value combined as Q-value\n",
    "    def forward(self, x):\n",
    "        x = self.pre(x)\n",
    "        val = self.val(x)\n",
    "        adv = self.adv(x)\n",
    "        return val + (adv - adv.mean())\n",
    "      \n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs)\n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else : \n",
    "            return out.argmax().item()\n",
    "            \n",
    "def train(q, q_target, memory, optimizer):\n",
    "    s,a,r,s_prime,done_mask, idx, weights = memory.sample(batch_size)\n",
    "    q_out = q(s)\n",
    "    q_a = q_out.gather(1,a)\n",
    "    max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "    target = r + gamma * max_q_prime * done_mask\n",
    "    # use mean squared error loss for each sample, weighted by buffer weights\n",
    "    loss =  (q_a - target.detach()).pow(2) * weights\n",
    "    # adjust priorities based on the loss the corresponding transition exhibited\n",
    "    prios = loss + 1e-5\n",
    "    # mean over all losses and back-propogate\n",
    "    loss = loss.mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # update buffer priorities\n",
    "    prios = prios.data.cpu()\n",
    "    memory.update(idx, prios.numpy()[0])\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuRYCjFvSyp0"
   },
   "source": [
    "Wrappers CITE THIS BEFORE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0lMa3oKSyPa"
   },
   "outputs": [],
   "source": [
    "# Wrappers from https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/blob/master/Chapter06/lib/wrappers.py under the MIT license\n",
    "\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "   def __init__(self, env=None):\n",
    "       super(FireResetEnv, self).__init__(env)\n",
    "       assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "       assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "   def step(self, action):\n",
    "       return self.env.step(action)\n",
    "   def reset(self):\n",
    "       self.env.reset()\n",
    "       obs, _, done, _ = self.env.step(1)\n",
    "       if done:\n",
    "          self.env.reset()\n",
    "       obs, _, done, _ = self.env.step(2)\n",
    "       if done:\n",
    "          self.env.reset()\n",
    "       return obs\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "           obs, reward, done, info = self.env.step(action)\n",
    "           self._obs_buffer.append(obs)\n",
    "           total_reward += reward\n",
    "           if done:\n",
    "               break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "    def reset(self):\n",
    "       self._obs_buffer.clear()\n",
    "       obs = self.env.reset()\n",
    "       self._obs_buffer.append(obs)\n",
    "       return obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ck-chjFdScJ"
   },
   "source": [
    "**Train**\n",
    "\n",
    "â† You can download the videos from the videos folder in the files on the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5stHkFq4UztI"
   },
   "outputs": [],
   "source": [
    "# setup the Gravitar ram environment, and record a video every 50 episodes. You can use the non-ram version here if you prefer\n",
    "env = gym.make('Gravitar-ram-v0')\n",
    "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda episode_id: (episode_id%video_every)==0,force=True)\n",
    "env = MaxAndSkipEnv(env)\n",
    "env = FireResetEnv(env)\n",
    "# reproducible environment and action spaces, do not change lines 6-11 here (tools > settings > editor > show line numbers)\n",
    "seed = 742\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "env.action_space.seed(seed)\n",
    "q = QNetwork().to(device)\n",
    "q_target = QNetwork().to(device)\n",
    "\n",
    "memory = ReplayBuffer()\n",
    "\n",
    "score    = 0.0\n",
    "marking  = []\n",
    "optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "frame_idx = 0\n",
    "episode = 0\n",
    "\n",
    "\n",
    "# Uncomment below line to resume training from saved model\n",
    "# load()\n",
    "\n",
    "\n",
    "q_target.load_state_dict(q.state_dict())\n",
    "for n_episode in range(episode, int(ep_limit)):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    score = 0.0\n",
    "    while True:\n",
    "        # model epsilon as an exponential decay with fixed min of 0.01 and max of 1.0, starts converging faster than linear decay in practice\n",
    "        # based on re-aranged equation found at https://math.stackexchange.com/questions/2362737/how-to-find-a-differential-equation-with-exponential-decay-between-two-values\n",
    "        epsilon =  0.01 + (1-0.01) * math.exp(-frame_idx/30000)\n",
    "        # do action based on current state\n",
    "        a = q.sample_action(torch.from_numpy(np.array(s)).float().unsqueeze(0).to(device), epsilon)\n",
    "        # done means episode is over\n",
    "        s_prime, r, done, info = env.step(a)\n",
    "        done_mask = 0.0 if done else 1.0\n",
    "        memory.put((s,a,r,s_prime, done_mask))\n",
    "        s = s_prime\n",
    "        score += r\n",
    "        if done:\n",
    "            break\n",
    "        if frame_idx % update_freq == 0:\n",
    "            q_target.load_state_dict(q.state_dict())\n",
    "        frame_idx += 1\n",
    "    # if we have a big enough history then train on it\n",
    "        if memory.size()>10000:\n",
    "            train(q, q_target, memory, optimizer)\n",
    "\n",
    "    # do not change lines 44-48 here, they are for marking the submission log\n",
    "    marking.append(score)\n",
    "    if n_episode%100 == 0:\n",
    "        print(\"marking, episode: {}, score: {:.1f}, mean_score: {:.2f}, std_score: {:.2f}\".format(\n",
    "            n_episode, score, np.array(marking).mean(), np.array(marking).std()))\n",
    "        marking = []\n",
    "\n",
    "    # you can change this part, and print any data you like (so long as it doesn't start with \"marking\")\n",
    "    if n_episode%print_every==0 and n_episode!=0:\n",
    "        print(\"episode: {}, score: {:.1f}, epsilon: {:.2f}\".format(n_episode, score, epsilon))\n",
    "    if n_episode%100==0 and n_episode!=0:\n",
    "        checkpoint(q, optimizer, frame_idx, n_episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TfAaC1B71ec2",
    "outputId": "d6b89978-4d20-40af-e752-80ed87ef75db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkxZc0Zu00Og"
   },
   "outputs": [],
   "source": [
    "# save progress\n",
    "import pickle\n",
    "def checkpoint(q,opt,frame, episode):\n",
    "  torch.save({'Q':q.state_dict(), 'optimiser':opt.state_dict(), 'e':episode, 'f':frame}, 'drive/My Drive/training/rl-cit.chkpt')\n",
    "  pickle.dump(memory, open('drive/My Drive/training/buffer-cit.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hwnlf5JWukAm"
   },
   "outputs": [],
   "source": [
    "# load saved model\n",
    "def load():\n",
    "    global frame_idx, episode, q, optimizer, memory\n",
    "    params = torch.load('drive/My Drive/training/rl-cit.chkpt')\n",
    "    frame_idx = params['f']\n",
    "    episode = params['e']\n",
    "    q.load_state_dict(params['Q'])\n",
    "    optimizer.load_state_dict(params['optimiser'])\n",
    "    memory = pickle.load(open('drive/My Drive/training/buffer-cit.p', 'rb'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DDQN_prio_noisy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
